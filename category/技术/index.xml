<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>技术 | 笼鹰的自习室</title><link>/category/%E6%8A%80%E6%9C%AF/</link><atom:link href="/category/%E6%8A%80%E6%9C%AF/index.xml" rel="self" type="application/rss+xml"/><description>技术</description><generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><copyright>This work by 笼鹰 is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License. 陕公网安备 61010302000452号 © 2020</copyright><lastBuildDate>Wed, 07 Oct 2020 00:00:00 +0000</lastBuildDate><image><url>/images/icon_hu0ad9a760d4acc21697f10ad9ba07207a_233385_512x512_fill_lanczos_center_2.png</url><title>技术</title><link>/category/%E6%8A%80%E6%9C%AF/</link></image><item><title>bazel编译onos出错</title><link>/post/onos/</link><pubDate>Wed, 07 Oct 2020 00:00:00 +0000</pubDate><guid>/post/onos/</guid><description>&lt;h2 id="编译错误npm安装失败webgui_onos-gui-npm-install-failed">编译错误，npm安装失败：&lt;code>//web/gui:_onos-gui-npm-install failed&lt;/code>&lt;/h2>
&lt;pre>&lt;code>ERROR: /home/onos/onos/web/gui/BUILD:96:1: Executing genrule //web/gui:_onos-gui-npm-install failed (Exit 1) bash failed: error executing command /bin/bash -c ... (remaining 1 argument(s) skipped)
Use --sandbox_debug to see verbose messages from the sandbox
Target //:onos failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 1991.490s, Critical Path: 1876.35s
INFO: 2419 processes: 2091 linux-sandbox, 328 worker.
FAILED: Build did NOT complete successfully
&lt;/code>&lt;/pre>
&lt;p>解决方法参考
&lt;a href="https://groups.google.com/a/onosproject.org/forum/#!topic/onos-dev/1fs11-NwIEk" target="_blank" rel="noopener">Google group&lt;/a> 或
&lt;a href="https://blog.csdn.net/daihanglai7622/article/details/88642998" target="_blank" rel="noopener">CSDN&lt;/a>&lt;/p>
&lt;ul>
&lt;li>单独编译以查看log
bazel build //web/gui:_onos-gui-npm-install &amp;ndash;verbose_failures &amp;ndash;sandbox_debug
sar -n DEV 1 2000000000000 查看网速，因为运行bazel脚本实际上就是在配置、编译和安装环境。这个过程是需要下载东西的。卡住不动要么是被墙了，要么是命令行权限或者缺少依赖的软件（用apt装一下）
sudo find / -name npm-install.out
cat日志发现
PhantomJS not found on PATH
Downloading &lt;a href="https://github.com/Medium/phantomjs/releases/download/v2.1.1/phantomjs-2.1.1-macosx.zip">https://github.com/Medium/phantomjs/releases/download/v2.1.1/phantomjs-2.1.1-macosx.zip&lt;/a>
Saving to /var/folders/mh/2ptfthxj2qb49jscj1b0gjsm0000gn/T/phantomjs/phantomjs-2.1.1-macosx.zip
Receiving&amp;hellip;
一直卡死这，直到报错。
原因是天朝网络：它的安装过程中要去 github 下载一个包，而 github release 文件放在亚马逊 aws 上（被墙了）。解决方法是借助淘宝镜像
修改web/gui/BUILD
&amp;quot; $$NPM $$NPM_ARGS install &amp;ndash;no-cache &amp;ndash;loglevel=error &amp;gt;npm-install.out 2&amp;gt;&amp;amp;1 &amp;amp;&amp;amp;&amp;quot;
的前面新增一行
&amp;quot; export PHANTOMJS_CDNURL=https://npm.taobao.org/mirrors/phantomjs &amp;amp;&amp;amp;&amp;quot;
也可以手动下载拷贝到/tmp/phantomjs下。因为每次重启电脑该目录都会被删除。
&lt;pre>&lt;code class="language-bash">cd ~/onos/tools/dev/bin/
./onos-gen-bazel-project &amp;gt; /tmp/onos_bazelproject
&lt;/code>&lt;/pre>
&lt;/li>
&lt;/ul>
&lt;h2 id="导入intellij-idea出错">导入intellij idea出错&lt;/h2>
&lt;p>如果是安装错误，是网络或墙
如果是编译错误，很可能是bazel插件版本不对，intellij idea版本和插件版本有对应关系
详情见：&lt;a href="https://plugins.jetbrains.com/plugin/8609-bazel/versions" target="_blank" rel="noopener">bazle plugin version&lt;/a>
java.lang.NoClassDefFoundError: com/google/common/util/concurrent/internal/InternalFutureFailureAccess
Caused by: java.lang.ClassNotFoundException: com.google.common.util.concurrent.internal.InternalFutureFailureAccess&lt;/p>
&lt;p>Did you find this page helpful? Consider sharing it 🙌&lt;/p></description></item><item><title>linux磁盘扩容</title><link>/post/disk/</link><pubDate>Wed, 07 Oct 2020 00:00:00 +0000</pubDate><guid>/post/disk/</guid><description>&lt;h3 id="背景">背景&lt;/h3>
&lt;p>作为Linux菜鸟，每次装Linux操作系统时我都要在分区这一步上花些功夫思考：怎么划分比较合适。（我的近期需求是什么?未来可能发生变化吗？）因为在逻辑分区划分好后就很难更改了，一旦某个分区空间耗尽会很麻烦。之前用EXSI管理服务器里的虚拟机，可以直接在web控制台中动态调整其磁盘大小。如果是vmware workstation pro或者virtualbox的虚拟机一般就用gparted调整分区(这么干有点害怕搞坏，操作前我会备份一下文件到移动硬盘)。自从学习了LVM后，分区管理一下子就变容易了，直接在CLI终端操作。&lt;/p>
&lt;h3 id="快速了解lvm">快速了解LVM&lt;/h3>
&lt;p>简单来说LVM就是将底层的多块物理卷PV组成一个存储池VG，用户可以在存储池上创建多个逻辑卷LV，这个逻辑卷就相当于windows下的一个C盘或者别的盘，用户可以往逻辑卷上装系统或者写数据，效果等同于写物理块上。&lt;/p>
&lt;p>LVM最关键的点就是这个逻辑卷，这是LVM好用的原因，因为LVM帮用户做好了映射。我们只需要知道LVM是利用Linux内核的device-mapper功能来实现存储系统的虚拟化，性能影响不大。LVM的很多好处都是因为有这个映射在。&lt;/p>
&lt;p>LVM最大的好处就是它可以在线（online）对逻辑卷（LV）和卷组（VG）进行创建、删除、调整大小等操作。无需重新启动服务，就可以将服务中用到的逻辑卷（LV）在线（online）/动态（live）迁移至别的硬盘上。&lt;/p>
&lt;p>当然还有些别的好处，例如允许创建快照，支持各种设备映射目标（device-mapper targets）。&lt;/p>
&lt;h3 id="稍微深入一下lvm">稍微深入一下LVM&lt;/h3>
&lt;ul>
&lt;li>MBR（Master Boot Record）（主引导记录）和GPT（GUID Partition Table）（GUID意为全局唯一标识符）是在磁盘上存储分区信息的两种不同方式。&lt;/li>
&lt;/ul>
&lt;ol>
&lt;li>传统的MBR分区方式最多4个主分区（3个主分区+1个扩展分区(扩展分区里面可以放多个逻辑分区)），无法创建大于2TB的分区，一般用fdisk分区工具。&lt;/li>
&lt;li>GPT分区方式将不会有这种限制，一般使用parted分区；&lt;/li>
&lt;/ol>
&lt;ul>
&lt;li>逻辑卷管理(LVM)，是 Logical Volume Manager（逻辑卷管理）的简写，lvm是卷的一种管理方式，并不是分区工具（也可不采用这种LVM管理方式）.&lt;/li>
&lt;/ul>
&lt;p>LVM的基本组成部分如下：&lt;/p>
&lt;ul>
&lt;li>物理卷 (PV)
&lt;br>一个可供存储LVM的块设备. 例如: 一块硬盘, 一个MBR或GPT分区, 一个回环文件, 一个被内核映射的设备 (例如 dm-crypt)。PV包含有与LVM相关的管理参数&lt;/li>
&lt;li>卷组 (VG)
&lt;br>&lt;font color=red>物理卷PV组合成一个存储池VG，作为存放逻辑卷LV的容器&lt;/font>。 PEs are allocated from a VG for a LV.&lt;/li>
&lt;li>&lt;strong>逻辑卷 (LV)&lt;/strong>
&lt;br>&lt;font color=red>&amp;ldquo;虚拟/逻辑卷&amp;quot;存放在一个卷组中,由物理块组成。&lt;/font>是一个类似于物理设备的块设备，例如，你可以直接在它上面创建一个文件系统文件系统。&lt;strong>逻辑卷不必是连续的空间，它可以跨越许多物理卷，并且可以在任何时候任意的调整大小。相比物理磁盘来说，更易于磁盘空间的管理&lt;/strong>&lt;/li>
&lt;li>物理块 (PE)
&lt;br>一个卷组中最小的连续区域(默认为4 MiB)，多个物理块将被分配给一个逻辑卷。可以把它看成物理卷的一部分，这部分可以被分配给一个逻辑卷。
&lt;img src="https://img.linux.net.cn/data/attachment/album/201406/18/134408sa12dauefffyszfg.jpg" alt="">&lt;/li>
&lt;/ul>
&lt;p>简单理解一下: 有一块物理硬盘/dev/sda，我们用LVM管理它。一个卷组VG可以看成是一个虚拟硬盘，逻辑卷LV可以看成虚拟硬盘上的磁盘分区，类似于C盘、D盘。而卷组VG是由下面的多个物理卷PV组成，物理卷PV的基本组成单位是物理块PE。而逻辑与物理实体之间映射关系不用管。&lt;/p>
&lt;p>&lt;img src="https://s2.ax1x.com/2020/01/22/1AMidK.png" alt="1AMidK.png">
如图，50G的硬盘/dev/sda，默认只分配了4G给根目录（红色实线），后面不够用了可以动态从VG里拨空间。&lt;/p>
&lt;p>下面再来看看LVM在baidu百科上的介绍&lt;font color="red">&lt;strong>LVM&lt;/strong>&lt;/font>(Logical Volume Manager)逻辑分卷管理器，本质上是一个虚拟设备驱动，是在内核中块设备和物理设备之间添加的一个新的抽象层次。&lt;/p>
&lt;h3 id="扩容和分区">扩容和分区&lt;/h3>
&lt;ol>
&lt;li>虚拟机关机，在宿主机的vmware界面中编辑虚拟机设置，硬件/硬盘/扩展磁盘容量。如果扩展选项是灰色的，需要删除快照或者使用vCenter Converter。如果宿主机是linux，可以用命令扩展磁盘：vmware-vdiskmanager -x 16Gb xxx.vmdk&lt;/li>
&lt;li>启动虚拟机&lt;/li>
&lt;/ol>
&lt;pre>&lt;code class="language-bash"># 查看硬盘容量
sudo fdisk -l
&amp;gt; Disk /dev/sda: 21.5 GB, 21474836480 bytes
# 查看文件系统
df -lh
&amp;gt; 和扩容之前结果一样，因为还没有给硬盘分区
# 开始分区
sudo fdisk /dev/sda
# 依次输入
m （命令提示）
p （查看分区）发现有三个盘，linux类型的主分区、Extended类型的、swap类型的，blocks为盘的大小（单位kB）
n （创建分区）
p (设为主分区)
3 (分区id)
起始扇区 需要观察之前查看分区的硬盘分布情况，选择有足够大空间的一段
结束扇区 直接回车，默认是将这段空白存储全部分给新分区
t （修改类型，8e为lvm，83为linux）
8e
w (写分区表)
# 分区结束，回到终端继续操作
sudo partprobe /dev/sda
sudo mkfs.ext4 /dev/sda3
&lt;/code>&lt;/pre>
&lt;h3 id="lvm卷管理">lvm卷管理&lt;/h3>
&lt;pre>&lt;code class="language-bash">sudo apt install lvm2
# 进入lvm管理
sudo lvm
# 查看物理卷PV
pvdisplay 或 pvscan
# 创建PV
pvcreate /dev/sda3
# 查看卷组VG
vgdisplay -v
# 创建VG，物理卷加入卷组
vgcreate vg /dev/sda3
# 创建逻辑卷LV
sudo lvcreate -n lv -L 12G vg
# 在LV上创建文件系统
sudo mkfs.ext4 /dev/ubuntu/lv
# 逻辑卷的UUID
blkid /dev/vg/lv
mkdir /home/vl
# 插入相应的条目/etc/fstab
UUID=b85df913-580f-461c-844f-546d8cde4646 /home/vl ext4 defaults 0 0
# 挂载
sudo mount /home/vl
&lt;/code>&lt;/pre>
&lt;p>用df -h和pvdisplay以及fdisk -l查看结果。&lt;/p>
&lt;blockquote>
&lt;p>Disk identifier: 0x00000000
Disk /dev/mapper/vg-lv doesn&amp;rsquo;t contain a valid partition table&lt;/p>
&lt;/blockquote>
&lt;p>这个是正常的，因为vg-lv是逻辑卷。本来就不应该有分区表的。&lt;/p>
&lt;h3 id="扩展根目录">扩展根目录&lt;/h3>
&lt;p>&lt;font color=red>前面是将整个虚拟机硬盘扩容，然后创建一个lvm类型的物理卷/dev/sda3，然后为它创建对应的vg，lv和文件系统格式化，并且动态调整lv的容量&lt;/font>&lt;/p>
&lt;div class="alert alert-note">
&lt;div>
如果安装系统时直接把lvm挂载在根，系统会自动创建lv和vg。在卷管理步骤中自己不用再创建LV和VG，只要创建PV。
&lt;/div>
&lt;/div>
&lt;pre>&lt;code class="language-bash"># 将新增的物理盘加入根目录所在的逻辑卷
vgextend ubuntu--vg-ubuntu--lv /dev/xxx
# 扩展文件系统所在逻辑卷组
lvextend -L +20G /dev/mapper/ubuntu--vg-ubuntu--lv #若提示No space left on device，考虑先暂时将本地没用的文件删除或移出到移动硬盘，扩容后移回
或者
lvextend -L +100% /dev/mapper/ubuntu--vg-ubuntu--lv #如果前面vmware新增了20G，这里新增20G到lv大概率会出错，实际能用到的空间会比之前的小一点点。不清楚，可以用百分数代替。
# 使扩容生效
sudo resize2fs -p /dev/mapper/ubuntu--vg-ubuntu--lv
&lt;/code>&lt;/pre>
&lt;p>如果安装系统时没有选lvm挂载根目录，桌面linux系统建议使用parted，它支持多种分区表类型，允许用户创建、删除、调整、缩减、移动和复制分区，以及重新组织硬盘的使用，复制数据到新的硬盘上。gparted 是 parted 的图形界面前端&lt;/p>
&lt;p>sudo apt-get install parted&lt;/p>
&lt;p>&lt;img src="https://s1.ax1x.com/2020/04/12/GLTotK.png" alt="GLTotK.png">&lt;/p>
&lt;p>如图所示，现在的情况不允许直接把unallocated的区间分配给sda1，因为区间不连续。移动区间步骤如下&lt;/p>
&lt;ol>
&lt;li>右击sda5(linux-swap)，点击swapoff。把🔒去除。接着进行2~4步将unallocated的区间逐步移到sda1(/)的旁边。&lt;/li>
&lt;li>右击sda2(extended)，将free space following设为0，或者拖动滑块。使得unallocated包含进sda2&lt;/li>
&lt;li>右击sda5，将free space following设为0。这时会弹出警告，点ok，忽略它。&lt;/li>
&lt;li>右击sda2，将new size设为linux-swap的大小，使得unllocated继续上移。&lt;/li>
&lt;li>右击sda1，由于空闲区间已经在旁边了，调整大小。&lt;/li>
&lt;li>点击apply all operations图标。&lt;/li>
&lt;/ol>
&lt;p>退出程序后，df -h查看。整个过程其实也没有重启。
&lt;img src="https://s1.ax1x.com/2020/04/12/GLbpQg.png" alt="GLbpQg.png">&lt;/p>
&lt;h3 id="参考资料">参考资料：&lt;/h3>
&lt;ol>
&lt;li>&lt;a href="https://gtcsq.readthedocs.io/en/latest/linux_tools/ubuntu_lvm_extend.html">https://gtcsq.readthedocs.io/en/latest/linux_tools/ubuntu_lvm_extend.html&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://baike.baidu.com/item/LVM">https://baike.baidu.com/item/LVM&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://www.cnblogs.com/hester/p/11439353.html">https://www.cnblogs.com/hester/p/11439353.html&lt;/a>
Did you find this page helpful? Consider sharing it 🙌&lt;/li>
&lt;/ol></description></item><item><title>ovs问题总结</title><link>/post/ovs/</link><pubDate>Wed, 07 Oct 2020 00:00:00 +0000</pubDate><guid>/post/ovs/</guid><description>&lt;h2 id="inport等于outport">inport等于outport&lt;/h2>
&lt;p>在k=6的胖树上做网络验证的实验时，发现了一种特殊的网络不可达故障。debug定位到可疑的交换机节点。最终分析出原因是编译器在两个相连的交换机之间存在来回的流表。&lt;/p>
&lt;p>由于用mininet构造了一个简单拓扑，快速验证我的猜想：交换机不能把数据包从inport口输出。&lt;/p>
&lt;p>&lt;a href="https://imgchr.com/i/8D0yXn" target="_blank" rel="noopener">&lt;img src="https://s1.ax1x.com/2020/03/18/8D0yXn.md.png" alt="8D0yXn.md.png">&lt;/a>&lt;/p>
&lt;p>我构造了一个简单的拓扑h1-&amp;gt;s1-&amp;gt;s2-&amp;gt;s1-&amp;gt;s3-&amp;gt;h2，发icmp包时发现手动安装的四条流表，只有s1、s2上分别匹配了一次，说明数据包并没有从s2回到s1，虽然匹配到s2上的inport=1, action=outport:1的流表。
google发现&lt;a href="https://mailman.stanford.edu/pipermail/openflow-discuss/2015-April/005636.html" target="_blank" rel="noopener">将actios改为IN_PORT可以解决问题0。&lt;/a>。&lt;/p>
&lt;p>虽然匹配了转发规则但是数据包却被丢弃的原因是OpenFlow 要求交换机不要把数据包从 ingress端口发出。貌似是为了避免转发环路出现。但是有时候确实有这个需求，而且不会造成问题。具体可以从链接中进去openflow论坛了解。&lt;/p>
&lt;h2 id="mininet配置网关">mininet配置网关&lt;/h2>
&lt;p>&lt;strong>ping不在一个网段的主机，包会转到子网的网关。而目前没有针对两个网段设置网关。&lt;/strong>&lt;/p>
&lt;pre>&lt;code class="language-bash">#修改host ip
py h1.setIP('10.0.0.11/24')
py h2.setIP('10.0.1.22/24')
#查看路由表
h1 netstat –rn或route(慢)
#给h1配置默认网关（ip）
h1 route add default gw 10.0.0.1 (h1-eth0)
#给h1配置静态arp表(默认网关的mac填host2)
h1 arp -s 10.0.0.1 00:00:00:00:00:02
h2 route add default gw 10.0.1.1
h2 arp -s 10.0.1.1 00:00:00:00:00:01
#查看arp表
h1 arp -a或-n
&lt;/code>&lt;/pre>
&lt;p>sudo tcpdump -e -vv -i s1-eth1（-vv详细报文信息，-e显示mac，–i指定监听的端口）&lt;/p>
&lt;p>h1 ping –c 1 h2
由于不是一个网段的，icmp报文的mac填的是网关的mac。&lt;/p>
&lt;pre>&lt;code class="language-bash">清空arp
h1 arp -n|awk '/^[1-9]/{print &amp;quot;arp -d &amp;quot; $1}'|sh -x
h2 arp -n|awk '/^[1-9]/{print &amp;quot;arp -d &amp;quot; $1}'|sh -x
&lt;/code>&lt;/pre>
&lt;p>ping不通是因为
&lt;strong>h1-eth0会发送ARP包获取网关的mac，而当前网关地址不存在对应设备，因此ICMP封包前获取不到目标地址信息，而导致网络不可达。&lt;/strong>&lt;/p>
&lt;p>（如果实验连接了控制器，可以通过控制器获得目标地址的信息，如OpenDaylight利用它预设的ARP Proxy。）&lt;/p>
&lt;p>让ovs的两个端口成为网关&lt;/p>
&lt;pre>&lt;code class="language-bash">mininet&amp;gt; s1 ifconfig s1-eth1 10.0.0.1/24
mininet&amp;gt; s1 ifconfig s1-eth2 10.0.1.1/24
&lt;/code>&lt;/pre>
&lt;p>配置流表让arp和icmp消息得到转发&lt;/p>
&lt;pre>&lt;code class="language-bash">处理ARP请求
当网管的ARP流到来后，将其交给本地的OVS处理。
mininet&amp;gt; sh ovs-ofctl add-flow s1 &amp;quot;table=0,priority=65535,arp,arp_tpa=10.0.0.1 actions=LOCAL&amp;quot;
mininet&amp;gt; sh ovs-ofctl add-flow s1 &amp;quot;table=0,priority=65535,arp,arp_tpa=10.0.1.1 actions=LOCAL&amp;quot;
处理ARP应答
该应答回复目标地址的出口，比如将目标网络10.0.0.1的包通过出口端口1抓发出去。端口信息可以查询如下：
mininet&amp;gt; sh ovs-vsctl -- --columns=name,ofport list Interface
name : &amp;quot;s1-eth2&amp;quot;
ofport : 2
name : &amp;quot;s1-eth1&amp;quot;
ofport : 1
name : &amp;quot;s1&amp;quot;
ofport : 65534
处理应答的流表如下：
mininet&amp;gt; sh ovs-ofctl add-flow s1 &amp;quot;table=0,priority=1,arp,nw_dst=10.0.0.1,actions=output:1&amp;quot;
mininet&amp;gt; sh ovs-ofctl add-flow s1 &amp;quot;table=0,priority=1,arp,nw_dst=20.0.0.1,actions=output:2&amp;quot;
对ICMP的请求进行处理。
为了使得流表表达更清晰，我们将ICMP路由的处理放在另外一个table处理。 也就是在table0中设置一个最低优先级的流，将非ARP的包丢给下一个流表处理。
mininet&amp;gt; sh ovs-ofctl add-flow s1 &amp;quot;table=0,priority=0,actions=resubmit(,1)&amp;quot;
在table(1)中，OVS的角色有点像router，我们需要修改ICMP封包的目标MAC地址。
mininet&amp;gt; sh ovs-ofctl add-flow s1 &amp;quot;table=1,icmp,nw_dst=10.0.0.1,actions=mod_dl_dst=00:00:00:00:00:01,output:1&amp;quot;
mininet&amp;gt; sh ovs-ofctl add-flow s1 &amp;quot;table=1,icmp,nw_dst=20.0.0.1,actions=mod_dl_dst=00:00:00:00:00:02,output:2&amp;quot;
&lt;/code>&lt;/pre>
&lt;p>##用Linux命令搭建sdn网络&lt;/p>
&lt;pre>&lt;code class="language-bash"># Create host namespaces
ip netns add h1
ip netns add h2
# Create switch
ovs-vsctl add-br s1
# Create links
ip link add h1-eth0 type veth peer name s1-eth1
ip link add h2-eth0 type veth peer name s1-eth2
ip link show
# Move host ports into namespaces
ip link set h1-eth0 netns h1
ip link set h2-eth0 netns h2
ip netns exec h1 ip link show
ip netns exec h2 ip link show
# Connect switch ports to OVS
ovs-vsctl add-port s1 s1-eth1
ovs-vsctl add-port s1 s1-eth2
ovs-vsctl show
# Set up OpenFlow controller
ovs-vsctl set-controller s1 tcp:127.0.0.1
ovs-controller ptcp: &amp;amp;
ovs-vsctl show
# Configure network
ip netns exec h1 ifconfig h1-eth0 10.1
ip netns exec h1 ifconfig lo up
ip netns exec h2 ifconfig h2-eth0 10.2
ip netns exec h2 ifconfig lo up
ifconfig s1-eth1 up
ifconfig s1-eth2 up
# Test network
ip netns exec h1 ping -c1 10.2
&lt;/code>&lt;/pre>
&lt;h2 id="mininet主机互相发包">mininet主机互相发包&lt;/h2>
&lt;p>在mininet/mininet/中的net.py中添加函数：&lt;/p>
&lt;pre>&lt;code class="language-python"> def sendMulti(self, number, period, times):
times=int(times)
#os.system(&amp;quot;ls&amp;quot;)
while (times):
base_port = 5001
host_list = [h for h in self.hosts]
_len = len(host_list)
for i in range(0, _len):
client = host_list[i]
for j in range(0, _len):
server = host_list[j]
if (i != j):
client.cmd(&amp;quot;/home/cjx/ubuntu/mininet/sends&amp;quot; + ' ' + server.IP() + ' ' + number + ' ' + period)
base_port += 1
times = times - 1
&lt;/code>&lt;/pre>
&lt;p>在mininet/mininet/中的cli.py中添加函数：&lt;/p>
&lt;pre>&lt;code class="language-python">def do_sendmulti(self, line):
&amp;quot;&amp;quot;&amp;quot;Multi iperf UDP test between nodes&amp;quot;&amp;quot;&amp;quot;
args = line.split()
if len(args) == 3:
udpBw = args[0]
period = args[1]
times = args[2]
err = False
self.mn.sendMulti(udpBw, period,times)
else:
error('invalid number of args: sendmulti udpBw \n' +
'udpBw examples: 1M\n')
&lt;/code>&lt;/pre>
&lt;p>在mininet/bin/mn文件中两处修改：&lt;/p>
&lt;pre>&lt;code class="language-python">TESTS = { name: True
for name in ( 'pingall', 'pingpair', 'iperf', 'iperfudp', 'sendmulti' ) }
ALTSPELLING = { 'pingall': 'pingAll', 'pingpair': 'pingPair',
'iperfudp': 'iperfUdp',
'sendmulti': 'sendMulti'}
&lt;/code>&lt;/pre>
&lt;p>send.c发包源文件编译成执行文件&lt;/p>
&lt;pre>&lt;code class="language-c">int main(int argc, char** argv)
{
///必须有目的ip的参数
if(argc != 4)
{
printf(&amp;quot;usage: send [ip] [number] [time interval(ms)]\n&amp;quot;);
return -1;
}
///根据参数组合出目的主机的ip地址
char dst_ip[20];
sprintf(dst_ip,&amp;quot;%s&amp;quot;,argv[1]);
int times = atoi(argv[2]);
int time_interval = atoi(argv[3]);
time_interval*=1000;
/// 初始化本机地址
struct sockaddr_in my_addr;
memset(&amp;amp;my_addr,0,sizeof(my_addr));
my_addr.sin_family = AF_INET; //Address family 指定为ipv4
my_addr.sin_addr.s_addr = INADDR_ANY; ////INADDR_ANY表示自动获取本机地址
my_addr.sin_port = htons(8889); ///端口号（本机的端口好表示用于发送的端口号）
///初始化目的主机地址
struct sockaddr_in dst_addr;
memset(&amp;amp;dst_addr,0,sizeof(dst_addr));
dst_addr.sin_family = AF_INET;
dst_addr.sin_addr.s_addr = inet_addr(dst_ip);
dst_addr.sin_port = htons(8888);
///创建套接字
int sockfd = socket(AF_INET,SOCK_DGRAM,0);
if(sockfd == -1)
{
perror(&amp;quot;create socket failed\n&amp;quot;);
return -1;
}
///绑定端口
if(bind(sockfd,(struct sockaddr*)&amp;amp;my_addr,sizeof(my_addr)) == -1)
{
perror(&amp;quot;bind failed\n&amp;quot;);
return -1;
}
unsigned char buf[16];
int i = 0;
for(i=0; i&amp;lt; 16; i++)
{
buf[i] = 0xff;
}
///发送数据包
for(i=0; i&amp;lt;times; i++)
{
usleep(time_interval);
// sleep(1);
printf(&amp;quot;sending packet %d to %s\n&amp;quot;,i,dst_ip);
if(sendto(sockfd,buf,sizeof(buf),0,(struct sockaddr*)&amp;amp;dst_addr,sizeof(dst_addr)) == -1)
{
perror(&amp;quot;send failed\n&amp;quot;);
return -1;
}
}
close(sockfd);
}
&lt;/code>&lt;/pre>
&lt;p>Did you find this page helpful? Consider sharing it 🙌&lt;/p></description></item><item><title>RouteFlow</title><link>/post/routeflow/</link><pubDate>Wed, 07 Oct 2020 00:00:00 +0000</pubDate><guid>/post/routeflow/</guid><description>&lt;h1 id="routeflow">Routeflow&lt;/h1>
&lt;p>在openflow网络之上提供虚拟IP路由服务的平台。它依赖于POX,OpenFlow,MongoDB,OVS,Quagga&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/routeflow/RouteFlow">https://github.com/routeflow/RouteFlow&lt;/a> 官方代码&lt;/li>
&lt;/ul>
&lt;h2 id="第三方代码">第三方代码&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="https://github.com/chesteve/RouteFlowVM">https://github.com/chesteve/RouteFlowVM&lt;/a> 虚拟机&lt;/li>
&lt;li>&lt;a href="https://github.com/call518/SDN-TEST">https://github.com/call518/SDN-TEST&lt;/a> ONOS OpenDaylight Routeflowd等&lt;/li>
&lt;li>&lt;a href="https://github.com/hungys/RoutingFlow(">https://github.com/hungys/RoutingFlow(&lt;/a>在Ryu上实现RIP,OSPF路由协议)&lt;/li>
&lt;/ul>
&lt;h2 id="三大核心组件">三大核心组件&lt;/h2>
&lt;ul>
&lt;li>&lt;strong>RFClient&lt;/strong> : 运行在VM（准确说是lxc容器）中的守护程序，监测Linux ARP ,路由表的改变。如果有更新，路由信息会被发送给RFserver。&lt;/li>
&lt;li>&lt;strong>RFServer&lt;/strong> : 一个独立的app，用来连接并管理运行在VM中的RFClient实例。RFserver负责维护RFclient实例、接口和对应交换机、端口的映射关系。它也和与RFproxy连接，来配置流表&lt;/li>
&lt;li>&lt;strong>RFProxy&lt;/strong> ：POX的一个app,通过openflow协议与ovs交互，监听RFserver的指令，&lt;/li>
&lt;/ul>
&lt;h2 id="vmrfclient和ovs的映射关系">VM（RFclient）和OVS的映射关系&lt;/h2>
&lt;p>配置被存储在MongoDB一个集合中&lt;/p>
&lt;ul>
&lt;li>VM 和OVS的映射关系&lt;/li>
&lt;li>virtual interface和交换机端口的映射关系beyond 1:1&lt;/li>
&lt;li>多个控制器管理网络&lt;/li>
&lt;/ul>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>格式&lt;/th>
&lt;th>类型&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>vm id, vm port, -, -, -, -, -&lt;/td>
&lt;td>闲置的client port&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>-, -, -, -, dp id, dp port, ct id&lt;/td>
&lt;td>闲置的datapath port&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>vm id, vm port, dp id, dp port, -, -, ct id&lt;/td>
&lt;td>client-datapath映射关系&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>vm id, vm port, dp id, dp port, vs id, vs port, ct id&lt;/td>
&lt;td>激活的 client-datapath&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>其他与虚拟交换机相关的域 (vs_*)在运行时定义. The ct_id field 标识交换机连接的控制器.(此机制允许多控制器）&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>状态图：
&lt;img src="https://raw.githubusercontent.com/wiki/routeflow/RouteFlow/images/rfstates.png" align="center" >&lt;/p>
&lt;p>当交换机连入网络，RFproxy会告知RFserver交换机的每个物理端口。这些datapath端口被server注册作为空闲的datapath端口或作为client-datapath关联；前者发生的条件是要么&lt;strong>正在被注册的datapath端口没有configuration&lt;/strong>，要么&lt;strong>已配置的client端口（与此datapath端口关联）尚未注册&lt;/strong>。 后者发生的条件是在&lt;strong>client端口（基于the configuration与此datapath关联）被注册为空闲&lt;/strong>。&lt;/p>
&lt;p>当一个RFclient启动，他会告诉server它的端口。这些client端口被server注册作为空闲的client端口或client-datapath关联。和上文的datapath端口行为描述类似。&lt;/p>
&lt;p>关联之后，RFserver要求client触发一个消息，这个消息将通过client连接的虚拟交换机到达proxy。这样proxy能感知client和虚拟交换机之间的连接，通知server。
然后server决定怎么处理这个信息。一般地，proxy会被命令重定向所有从虚拟交换机到与它关联的物理交换机的流，and vice-versa（反之亦然）.&lt;/p>
&lt;p>当交换机断开网络，它的端口从所有涉及到的关联中移除，留下空闲的client端口。如果datapath回来了，系统将把它当作新的datapath，如上文描述.&lt;/p>
&lt;h3 id="lxc简介">LXC简介&lt;/h3>
&lt;p>Linux Container容器是一种内核虚拟化技术，简称LXC。提供轻量级的虚拟化，以便隔离进程和资源，使用 LXC 的优点就是不需要安装太多的软件包，使用过程也不会占用太多的资源。LXC 在资源管理方面依赖 Linux 内核的 cgroups （Control Groups） 系统，cgroups 系统是 Linux 内核提供的一个基于进程组的资源管理的框架，可以为特定的进程组限定可以使用的资源。&lt;/p>
&lt;h3 id="lxc常用命令">lxc常用命令&lt;/h3>
&lt;pre>&lt;code class="language-sh">1. 安装lxc
apt-get -y --force-yes install lxc
2. 检查
lxc-checkconfig 如果所有项目显示“enabled”则OK
3. 根据模板创建容器
lxc-create -t ubuntu -n base
#-n是容器的名字;-t 是容器的模板（模板的保存路径是：/usr/lib/lxc/templates/==模板都是一个脚本文件，执行了一系列操作，包括创建容器的时候挂载文件系统，配置网络，安装必要软件，创建用户/属组，设置密码等=
4. 启动 LXC 虚拟计算机
lxc-start -n testA 确认账号和密码后登录虚拟计算机
6、列出当前所有的容器
　　lxc-ls
7、使用 console 登入容器
lxc-console -n testA –t 3
8、停止运行一个容器
lxc-stop -n testA
9、获取一个容器的状态
lxc-info -n ol6ctr1
10、把一个容器销毁
lxc-destroy -n testA1
11、复制一个容器
lxc-clone -o testA -n ol6ctr2
12、暂停或恢复一个容器
lxc-freeze -n testA
lxc-unfreeze -n testA
13、修改 LXC 网络接口
vi /etc/default/lxc
14. 用户修改后要重新启动网络服务
service lxc-net restart
&lt;/code>&lt;/pre>
&lt;h2 id="实验">实验&lt;/h2>
&lt;h3 id="编译rfclient">编译rfclient&lt;/h3>
&lt;pre>&lt;code class="language-sh">make rfclient
&lt;/code>&lt;/pre>
&lt;p>#make build -&amp;gt; lib -&amp;gt; rfclient:&lt;br> 编译rflib/ipc, rflib/types, rfclient/. 目录下的文件到 build/&lt;/p>
&lt;pre>&lt;code class="language-sh">./Makefile
export ROOT_DIR=.
export BUILD_DIR=./build
export LIB_DIR=./rflib
export MONGO_DIR=/usr/local/include/mongo
# 编译产物存放目录
export BUILD_LIB_DIR=./build/lib
export BUILD_OBJ_DIR=./build/obj
export libdirs := ipc types （rflib下的两个目录）
&lt;/code>&lt;/pre>
&lt;h3 id="创建lxc容器切到rftest目录执行create">创建LXC容器：切到rftest目录,执行./create&lt;/h3>
&lt;p>create脚本内容：&lt;/p>
&lt;ol>
&lt;li>容器创建路径/var/lib/lxc&lt;/li>
&lt;li>apt安装lxc&lt;/li>
&lt;li>创建默认LXC容器base&lt;/li>
&lt;/ol>
&lt;pre>&lt;code>lxc-create -t ubuntu -n base
-n是容器的名字;-t 是容器的模板（在/usr/lib/lxc/templates/里，模板就是一个脚本文件，内容包括创建容器的时候挂载文件系统，配置网络，安装必要软件，创建用户/属组，设置密码等==）
&lt;/code>&lt;/pre>
&lt;ol start="4">
&lt;li>安装quagga、tcpdump到容器&lt;/li>
&lt;/ol>
&lt;pre>&lt;code class="language-sh">chroot $LXCDIR/base/rootfs apt-get install
#chroot命令用来在指定的根目录下运行指令
&lt;/code>&lt;/pre>
&lt;ol start="5">
&lt;li>基于刚刚创建的base容器和rftest/conifg下的容器配置文件，克隆其他LXC容器&lt;/li>
&lt;/ol>
&lt;h3 id="1rftest1">1)rftest1&lt;/h3>
&lt;p>创建3个LXC容器b1(172.31.1.2)、b2(172.31.2.2)、rfvm1（192.169.1.100）其中rfvm1分别配两个interface与b1,b2连接。设法使rfvm1充当网关（路由器），使得b1 能ping 通 b2。
&lt;img src="https://github.com/routeflow/RouteFlow/wiki/images/rftest1_scenario.png" alt="image">&lt;/p>
&lt;p>RF中的一台虚拟机充当openflow交换机，这台虚拟机中路由表和arp表发生任何事，都被复制到交换机。 RFClient负责监听这些表的事件并且通知rfserver。
&lt;br>因此要部署一台LXC容器,另配一个iface作为管理接口连接这个RFclient和其他&lt;br>
&lt;strong>config文件:&lt;/strong>
此文件是所有setup的基础，根据情况修改名字和hwaddr&lt;br>
第一个iface(rfvm1.0)的hwaddr将被用为RFclient的ID.&lt;/p>
&lt;pre>&lt;code class="language-sh">lxc.utsname = rfvm1
lxc.network.type = veth
lxc.network.flags = up
lxc.network.hwaddr = 12:a0:a0:a0:a0:a0
lxc.network.link=lxcbr0
lxc.network.type = veth
lxc.network.flags = up
lxc.network.veth.pair= rfvm1.1
lxc.network.hwaddr = 12:a1:a1:a1:a1:a1
lxc.network.type = veth
lxc.network.flags = up
lxc.network.veth.pair = rfvm1.2
lxc.network.hwaddr = 12:a2:a2:a2:a2:a2
...
lxc.rootfs = /var/lib/lxc/rfvm1/rootfs
lxc.mount = /var/lib/lxc/rfvm1/fstab
&lt;/code>&lt;/pre>
&lt;p>rootfs和mount不用修改。容器的位置一般在 &lt;strong>/var/lib/lxc&lt;/strong>
&lt;strong>/etc/sysctl.conf&lt;/strong> 中开启IPv4转发，让rfvm1作为网关，关闭quagga
&lt;strong>/etc/rc.local&lt;/strong>调用&lt;strong>root/run_rfclient脚本&lt;/strong>开启RFclient实例。&lt;/p>
&lt;p>==ReadMe==：All these customizations, when structured like in the rftest/config/rfvm1 folder will be read the the rftest/create script, that will create any number of LXC containers based on a basic template.&lt;/p>
&lt;h4 id="执行rftest1">执行./rftest1&lt;/h4>
&lt;p>此脚本必须被root用户执行。&lt;br>
一、重置环境:&lt;/p>
&lt;pre>&lt;code class="language-sh">reset 1：
初始化ovs:删除dp0,switch1网桥
初始化VMs：lxc关机，删除vm路径下的rootfs/var/run/network/ifstate
断开mongodb，删除数据rfvm1路径下的rootfs/opt/rfclient
trap &amp;quot;reset 0; exit 0&amp;quot; INT
#当脚本程序收到INT命令（被中断时），执行reset 0; exit 0完成清理工作
配置容器的管理接口：ifconfig lxcbr0 $MONGODB_ADDR(192.168.10.1) up
sed -i &amp;quot;/bind_ip/c\bind_ip = 127.0.0.1,$MONGODB_ADDR&amp;quot; $MONGODB_CON
#用bind_ip=127.0.0.1,192.168.10.1取代bind_ip行
重启mongodb
wait_port_listen $MONGODB_PORT(27017)
# 阻塞执行直到在指定端口监听到一个socket开启。
# MongoDB服务器：监听此网桥，确保容器能连接到 Routeflow架构。
&lt;/code>&lt;/pre>
&lt;p>二、配置LXC 容器&lt;strong>rfvm1&lt;/strong>作为RFclient运行&lt;/p>
&lt;pre>&lt;code>新建rfclient目录
mkdir /var/lib/lxc/rfvm1/rootfs/opt/rfclient
复制rfclient可执行程序
cp build/rfclient /var/lib/lxc/rfvm1/rootfs/opt/rfclient/rfclient
创建脚本run_rfclient.sh在/var/lib/lxc/rfvm1/rootfs/root下用于启动RFclient （脚本内容：sleep 5
/opt/rfclient/rfclient&amp;gt; /var/log/rfclient.log）
启动pox
#作为控制器app的RFproxy也随之启动,控制器日志等级设为INFO
wait_port_listen $Contorller_PORT(6633)
启动rfserver
./rfserver/rfserver.py rftest/rftest1config.csv &amp;amp;
# rftest1config.csv是vm_id,vm_port,ct_id,dp_id,dp_port映射表
启动容器并登录：lxc-start -n rfvm1 -d
&lt;/code>&lt;/pre>
&lt;p>三、连接容器到正在运行数据库服务的hosts&lt;/p>
&lt;pre>&lt;code class="language-sh">ovs添加网桥dp0，端口rfvm1.1，rfvm1.2,dpid=7266767372667673
dp0连接控制器：ovs-vsctl set-controller dp0 tcp:127.0.0.1:$CONTROLLER_PORT（6633）
启动hosts（容器b1、b2)
ovs添加网桥switch1,端口b1.0、b2.0，dpid=99
switch1连接到控制器
&lt;/code>&lt;/pre>
&lt;p>在host创建ovs网桥，连接&lt;em>容器的管理接口&lt;/em>（rfvm1的eth0）
&lt;strong>br0&lt;/strong>，192.169.1.1，这个地址被写死在rflib/defs.h（给RFclient和NOX-RFproxy以及RFserver和POX-RFproxy）
四、登录到容器host1
lxc-console -n b1（用户名密码都是ubuntu）
在b1（172.31.1.2）里ping -c 3 172.31.2.2（b2）&lt;/p>
&lt;p>ping通的原因：
rfserver维护映射表&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>vm_id&lt;/th>
&lt;th>vm_port&lt;/th>
&lt;th>ct_id&lt;/th>
&lt;th>dp_id,dp_port&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>12A0A0A0A0A0&lt;/td>
&lt;td>1&lt;/td>
&lt;td>0&lt;/td>
&lt;td>99&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>12A0A0A0A0A0&lt;/td>
&lt;td>2&lt;/td>
&lt;td>0&lt;/td>
&lt;td>99&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>容器id是12A0A0A0A0A0，它的接口eth1对应dpid（switch）的接口1,&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>接口eth2对应dpid（switch）的接口2&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>Did you find this page helpful? Consider sharing it 🙌&lt;/p></description></item><item><title>个人博客维护出现的问题</title><link>/post/website/</link><pubDate>Wed, 07 Oct 2020 00:00:00 +0000</pubDate><guid>/post/website/</guid><description>&lt;h2 id="hexo站点打开显示hsts问题">hexo站点打开显示HSTS问题&lt;/h2>
&lt;p>今天访问自己的博客网站时，浏览器并没有直接出现博客首页，而是出现了HSTS的页面（这里猜测应该是证书出了问题）。
使用chrome://net-internals/#hsts删除域名的安全策略。刷新后，点击访问不安全的网站，才进入了博客首页。但是发现浏览器的地址栏左侧显示红色的不安全，点击查看详情果然是证书有问题。&lt;/p>
&lt;p>这里得简要说明一下博主的网站搭建方式：是用hexo搭建的静态网站，源码托管在github上，采用travis CI进行持续部署与集成，即当此github项目更新时travis CI会执行.travis脚本的命令，
自动进行hexo的重新构建并部署，把生成的前端代码作为项目一式两分提交到github和coding托管。
分别启动coding和github的page服务，并且强行开启HTTPS。这时候通过&lt;username>.github.io和&lt;username>.coding,me就能访问了。
然后万网申请了一个域名，分别回到coding和github配置page服务绑定自定义域名。
然后在网站DNS解析控制台配置国内外分流，即国内ip访问域名进入的是coding的page，国外ip访问的是github的page。结束。&lt;/p>
&lt;p>所以现在网站出现问题，首先验证一下猜测。于是浏览器切换到代理模式，刷新博客首页，一切正常。这说明仅仅是coding的page服务出现了问题。于是访问coding发现page设置里的SSL/TLS安全证书状态显示错误。我先点击了申请，重试，仍然是错误，鼠标放到错误上显示的详情是urn:acme:error:unauthorized invalid response from [http://exmaple.com/.well-known/acme-challenge/xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx]&lt;/p>
&lt;h3 id="分析">分析&lt;/h3>
&lt;p>查阅coding的&lt;a href="https://dev.tencent.com/help/doc/faq/coding-pages/ssh/tls#urnacmeerrorunauthorized" target="_blank" rel="noopener">自定义域名的帮助文档&lt;/a>后才知道这是无法获取正确的域名验证信息。可能有两种原因：&lt;/p>
&lt;ul>
&lt;li>DNS 的 CNAME 记录是否设置正确&lt;/li>
&lt;li>&lt;strong>域名的 DNS 将海外线路解析到 Coding Pages 的服务器&lt;/strong>&lt;/li>
&lt;/ul>
&lt;p>考虑到之前网站访问都正常，一直没有修改过DNS设置，所以不太可能是第一个原因。按照第二个原因推测可能是coding证书过期了，重新申请的时候域名解析没有使用正确的分线路解析。注意到上面的xxxxxx中包含一个IP地址。这个很可能是网站提供page服务的服务器地址。于是ping &lt;username>.github.io和&lt;username>.coding.me查看域名服务器的ip，发现果然。那个ip正好是&lt;username>.github.io的域名ip。这就可以解释了，coding的证书申请，返回的却是github的地址。所以域，。&lt;/p>
&lt;p>找到问题所在后，控制台，先暂时将解析到github的两条记录暂停，只返回coding Pages界面，再回到coding重新申请证书。提示30分钟只能申请一次。好吧。于是把这个添加到TODO List。后面回来申请SSL/TLS安全证书的状态就变成了正常。&lt;/p>
&lt;h2 id="academic-theme-for-hugo">Academic theme for Hugo&lt;/h2>
&lt;ol>
&lt;li>更新主题失败&lt;/li>
&lt;/ol>
&lt;p>在脚本update_academic.sh的view_update中加入git submodule init即可
2. 关于academic主题绝大多数问题都可以在&lt;a href="https://github.com/gcushen/hugo-academic/issues" target="_blank" rel="noopener">hugo-academic-issues&lt;/a>中得到解答。不得不承认这是hugo里维护最好的一个项目，没有之一。&lt;/p>
&lt;p>Did you find this page helpful? Consider sharing it 🙌&lt;/p></description></item><item><title>加快apt/maven/bazel/npm/pip拉取速度</title><link>/post/mirror/</link><pubDate>Wed, 07 Oct 2020 00:00:00 +0000</pubDate><guid>/post/mirror/</guid><description>&lt;h2 id="apt">&lt;code>APT&lt;/code>&lt;/h2>
&lt;pre>&lt;code class="language-shell">deb http://us.archive.ubuntu.com/ubuntu/ bionic main restricted universe multiverse
deb http://us.archive.ubuntu.com/ubuntu/ bionic-security main restricted universe multiverse
deb http://us.archive.ubuntu.com/ubuntu/ bionic-updates main restricted universe multiverse
deb http://us.archive.ubuntu.com/ubuntu/ bionic-proposed main restricted universe multiverse
deb http://us.archive.ubuntu.com/ubuntu/ bionic-backports main restricted universe multiverse
deb-src http://us.archive.ubuntu.com/ubuntu/ bionic main restricted universe multiverse
deb-src http://us.archive.ubuntu.com/ubuntu/ bionic-security main restricted universe multiverse
deb-src http://us.archive.ubuntu.com/ubuntu/ bionic-updates main restricted universe multiverse
deb-src http://us.archive.ubuntu.com/ubuntu/ bionic-proposed main restricted universe multiverse
deb-src http://us.archive.ubuntu.com/ubuntu/ bionic-backports main restricted universe multiverse
&lt;/code>&lt;/pre>
&lt;p>注意：需要根据需求来改source.list。如：将us替换成cn即变成中国；将bionic替换成对应的ubuntu版本代号；对于镜像网站用mirros.&amp;lt;域名&amp;gt;替换us.archive.ubuntu.com；&lt;/p>
&lt;pre>&lt;code class="language-shell">deb http://mirrors.xjtu.edu.cn/ubuntu/ bionic main multiverse restricted universe
deb http://mirrors.xjtu.edu.cn/ubuntu/ bionic-backports main multiverse restricted universe
deb http://mirrors.xjtu.edu.cn/ubuntu/ bionic-proposed main multiverse restricted universe
deb http://mirrors.xjtu.edu.cn/ubuntu/ bionic-security main multiverse restricted universe
deb http://mirrors.xjtu.edu.cn/ubuntu/ bionic-updates main multiverse restricted universe
deb-src http://mirrors.xjtu.edu.cn/ubuntu/ bionic main multiverse restricted universe
deb-src http://mirrors.xjtu.edu.cn/ubuntu/ bionic-backports main multiverse restricted universe
deb-src http://mirrors.xjtu.edu.cn/ubuntu/ bionic-proposed main multiverse restricted universe
deb-src http://mirrors.xjtu.edu.cn/ubuntu/ bionic-security main multiverse restricted universe
deb-src http://mirrors.xjtu.edu.cn/ubuntu/ bionic-updates main multiverse restricted universe
&lt;/code>&lt;/pre>
&lt;pre>&lt;code class="language-shell"># newer versions of the distribution.
deb http://mirrors.aliyun.com/ubuntu bionic main restricted
# deb-src http://mirrors.aliyun.com/ubuntu bionic main restricted
## Major bug fix updates produced after the final release of the
## distribution.
deb http://mirrors.aliyun.com/ubuntu bionic-updates main restricted
# deb-src http://mirrors.aliyun.com/ubuntu bionic-updates main restricted
## N.B. software from this repository is ENTIRELY UNSUPPORTED by the Ubuntu
## team. Also, please note that software in universe WILL NOT receive any
## review or updates from the Ubuntu security team.
deb http://mirrors.aliyun.com/ubuntu bionic universe
# deb-src http://mirrors.aliyun.com/ubuntu bionic universe
deb http://mirrors.aliyun.com/ubuntu bionic-updates universe
# deb-src http://mirrors.aliyun.com/ubuntu bionic-updates universe
## N.B. software from this repository is ENTIRELY UNSUPPORTED by the Ubuntu
## team, and may not be under a free licence. Please satisfy yourself as to
## your rights to use the software. Also, please note that software in
## multiverse WILL NOT receive any review or updates from the Ubuntu
## security team.
deb http://mirrors.aliyun.com/ubuntu bionic multiverse
# deb-src http://mirrors.aliyun.com/ubuntu bionic multiverse
deb http://mirrors.aliyun.com/ubuntu bionic-updates multiverse
# deb-src http://mirrors.aliyun.com/ubuntu bionic-updates multiverse
## N.B. software from this repository may not have been tested as
## extensively as that contained in the main release, although it includes
## newer versions of some applications which may provide useful features.
## Also, please note that software in backports WILL NOT receive any review
## or updates from the Ubuntu security team.
deb http://mirrors.aliyun.com/ubuntu bionic-backports main restricted universe multiverse
# deb-src http://mirrors.aliyun.com/ubuntu bionic-backports main restricted universe multiverse
## Uncomment the following two lines to add software from Canonical's
## 'partner' repository.
## This software is not part of Ubuntu, but is offered by Canonical and the
## respective vendors as a service to Ubuntu users.
# deb http://archive.canonical.com/ubuntu bionic partner
# deb-src http://archive.canonical.com/ubuntu bionic partner
deb http://mirrors.aliyun.com/ubuntu bionic-security main restricted
# deb-src http://mirrors.aliyun.com/ubuntu bionic-security main restricted
deb http://mirrors.aliyun.com/ubuntu bionic-security universe
# deb-src http://mirrors.aliyun.com/ubuntu bionic-security universe
deb http://mirrors.aliyun.com/ubuntu bionic-security multiverse
# deb-src http://mirrors.aliyun.com/ubuntu bionic-security multiverse
&lt;/code>&lt;/pre>
&lt;h2 id="maven">Maven&lt;/h2>
&lt;p>在.m2/setting.xml替换对应部分为以下内容：&lt;/p>
&lt;pre>&lt;code class="language-xml">&amp;lt;?xml version=&amp;quot;1.0&amp;quot; encoding=&amp;quot;UTF-8&amp;quot;?&amp;gt;
&amp;lt;settings xmlns=&amp;quot;http://maven.apache.org/SETTINGS/1.0.0&amp;quot;
xmlns:xsi=&amp;quot;http://www.w3.org/2001/XMLSchema-instance&amp;quot;
xsi:schemaLocation=&amp;quot;http://maven.apache.org/SETTINGS/1.0.0 http://maven.apache.org/xsd/settings-1.0.0.xsd&amp;quot;&amp;gt;
&amp;lt;!--
&amp;lt;localRepository&amp;gt;~/.m2&amp;lt;/localRepository&amp;gt;
--&amp;gt;
&amp;lt;pluginGroups&amp;gt;
&amp;lt;/pluginGroups&amp;gt;
&amp;lt;proxies&amp;gt;
&amp;lt;/proxies&amp;gt;
&amp;lt;servers&amp;gt;
&amp;lt;/servers&amp;gt;
&amp;lt;!--
&amp;lt;mirrorOf&amp;gt;central&amp;lt;/mirrorOf&amp;gt; 这里最好不要写成*号，否则你项目中的pom中配置repository不生效
--&amp;gt;
&amp;lt;mirrors&amp;gt;
&amp;lt;mirror&amp;gt;
&amp;lt;id&amp;gt;nexus-aliyun&amp;lt;/id&amp;gt;
&amp;lt;mirrorOf&amp;gt;central&amp;lt;/mirrorOf&amp;gt;
&amp;lt;name&amp;gt;Nexus Aliyun&amp;lt;/name&amp;gt;
&amp;lt;url&amp;gt;http://maven.aliyun.com/nexus/content/groups/public/&amp;lt;/url&amp;gt;
&amp;lt;/mirror&amp;gt;
&amp;lt;/mirrors&amp;gt;
&amp;lt;profiles&amp;gt;
&amp;lt;profile&amp;gt;
&amp;lt;repositories&amp;gt;
&amp;lt;repository&amp;gt;
&amp;lt;id&amp;gt;nexus&amp;lt;/id&amp;gt;
&amp;lt;name&amp;gt;local private nexus&amp;lt;/name&amp;gt;
&amp;lt;url&amp;gt;http://maven.aliyun.com/nexus/content/groups/public/&amp;lt;/url&amp;gt;
&amp;lt;releases&amp;gt;
&amp;lt;enabled&amp;gt;true&amp;lt;/enabled&amp;gt;
&amp;lt;/releases&amp;gt;
&amp;lt;snapshots&amp;gt;
&amp;lt;enabled&amp;gt;false&amp;lt;/enabled&amp;gt;
&amp;lt;/snapshots&amp;gt;
&amp;lt;/repository&amp;gt;
&amp;lt;/repositories&amp;gt;
&amp;lt;pluginRepositories&amp;gt;
&amp;lt;pluginRepository&amp;gt;
&amp;lt;id&amp;gt;nexus&amp;lt;/id&amp;gt;
&amp;lt;name&amp;gt;local private nexus&amp;lt;/name&amp;gt;
&amp;lt;url&amp;gt;http://maven.aliyun.com/nexus/content/groups/public/&amp;lt;/url&amp;gt;
&amp;lt;releases&amp;gt;
&amp;lt;enabled&amp;gt;true&amp;lt;/enabled&amp;gt;
&amp;lt;/releases&amp;gt;
&amp;lt;snapshots&amp;gt;
&amp;lt;enabled&amp;gt;true&amp;lt;/enabled&amp;gt;
&amp;lt;/snapshots&amp;gt;
&amp;lt;/pluginRepository&amp;gt;
&amp;lt;/pluginRepositories&amp;gt;
&amp;lt;/profile&amp;gt;
&amp;lt;/profiles&amp;gt;
&amp;lt;/settings&amp;gt;
&lt;/code>&lt;/pre>
&lt;h2 id="pip">pip&lt;/h2>
&lt;p>对某个包使用清华源：sudo pip2 install -i &lt;a href="https://pypi.tuna.tsinghua.edu.cn/simple">https://pypi.tuna.tsinghua.edu.cn/simple&lt;/a> [package-name]&lt;/p>
&lt;p>要一直生效用命令存在配置文件中 pip3 config set global.index-url &lt;a href="https://pypi.tuna.tsinghua.edu.cn/simple">https://pypi.tuna.tsinghua.edu.cn/simple&lt;/a>&lt;/p>
&lt;h2 id="bazel和npm">bazel和npm&lt;/h2>
&lt;p>添加&lt;/p>
&lt;p>&amp;ndash;registry &lt;a href="https://registry.npm.taobao.org">https://registry.npm.taobao.org&lt;/a> 对编译时下载的所有依赖使用淘宝镜像&lt;/p>
&lt;p>或者对部分依赖使用镜像，例如phantomjs&lt;/p>
&lt;p>export PHANTOMJS_CDNURL=https://npm.taobao.org/mirrors/phantomjs&lt;/p>
&lt;h2 id="我的bash环境变量配置">我的bash环境变量配置&lt;/h2>
&lt;pre>&lt;code>export PROXY_IP=127.0.0.1
export PROXY_PORT=8118
export JAVA_HOME=/usr/lib/jvm/java-8-oracle
#export JAVA_HOME=&amp;quot;$(dirname $(dirname $(realpath $(which javac))))&amp;quot;
#export M2_HOME=/usr/lib/mvn/mvn
#export GOPATH=$HOME/go
#export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
export JRE_HOME=$JAVA_HOME/jre
export CLASSPATH=.:$JAVA_HOME/lib:/lib:
export M2_HOME=/usr/share/maven
export MAVEN_OPTS=&amp;quot;-Xms256m&amp;quot;
# WORKSPACE
export ONOS_ROOT=~/onos
source $ONOS_ROOT/tools/dev/bash_profile
export PATH=&amp;quot;$PATH:%HOME/bin&amp;quot;
export LC_ALL=&amp;quot;en_US.UTF-8&amp;quot;
# alias
alias speed=&amp;quot;watch -n 1 \&amp;quot;ifconfig ens33 |grep bytes\&amp;quot;&amp;quot;
alias localproxy=&amp;quot;sudo systemctl start v2ray.service ; sudo systemctl start privoxy.service&amp;quot;
alias localunproxy=&amp;quot;sudo systemctl stop v2ray.service ; sudo systemctl stop privoxy.service&amp;quot;
alias proxy=&amp;quot;export HTTP_PROXY=http://$PROXY_IP:$PROXY_PORT; export HTTPS_PROXY=https://$PROXY_IP:$PROXY_PORT; export http_proxy=http://$PROXY_IP:$PROXY_PORT; export https_proxy=https://$PROXY_IP:$PROXY_PORT&amp;quot;
alias unproxy=&amp;quot;unset HTTP_PRPXY; unset HTTPS_PROXY&amp;quot;
alias fishunproxy=&amp;quot;set -e HTTP_PROXY ; set -e HTTPS_PROXY; set -e http_proxy; set -e https_proxy&amp;quot;
alias gitproxy=&amp;quot;git config --global http.proxy http://$PROXY_IP:$PROXY_PORT;git config --global https.proxy http://$PROXY_IP:$PROXY_PORT &amp;quot;
alias gitunproxy=&amp;quot;git config --global --unset-all http.proxy;git config --global --unset-all https.proxy &amp;quot;
alias chrome=&amp;quot;chromium-browser --proxy-server=socks5://127.0.0.1:1080&amp;quot;
alias pipmirror=&amp;quot;pip3 config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple&amp;quot;
alias pipunmirror=&amp;quot;pip3 config unset global.index-url&amp;quot;
alias myip=&amp;quot;curl cip.cc&amp;quot;
#alias myip=&amp;quot;curl ifconfig.cc&amp;quot;
&lt;/code>&lt;/pre>
&lt;p>Did you find this page helpful? Consider sharing it 🙌&lt;/p></description></item></channel></rss>