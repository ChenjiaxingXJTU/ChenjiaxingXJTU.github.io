<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>ovs | 笼鹰的自习室</title><link>/tag/ovs/</link><atom:link href="/tag/ovs/index.xml" rel="self" type="application/rss+xml"/><description>ovs</description><generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><copyright>This work by 笼鹰 is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License. 陕公网安备 61010302000452号 © 2020</copyright><lastBuildDate>Wed, 07 Oct 2020 00:00:00 +0000</lastBuildDate><image><url>/images/icon_hu0ad9a760d4acc21697f10ad9ba07207a_233385_512x512_fill_lanczos_center_2.png</url><title>ovs</title><link>/tag/ovs/</link></image><item><title>ovs问题总结</title><link>/post/ovs/</link><pubDate>Wed, 07 Oct 2020 00:00:00 +0000</pubDate><guid>/post/ovs/</guid><description>&lt;h2 id="inport等于outport">inport等于outport&lt;/h2>
&lt;p>在k=6的胖树上做网络验证的实验时，发现了一种特殊的网络不可达故障。debug定位到可疑的交换机节点。最终分析出原因是编译器在两个相连的交换机之间存在来回的流表。&lt;/p>
&lt;p>由于用mininet构造了一个简单拓扑，快速验证我的猜想：交换机不能把数据包从inport口输出。&lt;/p>
&lt;p>&lt;a href="https://imgchr.com/i/8D0yXn" target="_blank" rel="noopener">&lt;img src="https://s1.ax1x.com/2020/03/18/8D0yXn.md.png" alt="8D0yXn.md.png">&lt;/a>&lt;/p>
&lt;p>我构造了一个简单的拓扑h1-&amp;gt;s1-&amp;gt;s2-&amp;gt;s1-&amp;gt;s3-&amp;gt;h2，发icmp包时发现手动安装的四条流表，只有s1、s2上分别匹配了一次，说明数据包并没有从s2回到s1，虽然匹配到s2上的inport=1, action=outport:1的流表。
google发现&lt;a href="https://mailman.stanford.edu/pipermail/openflow-discuss/2015-April/005636.html" target="_blank" rel="noopener">将actios改为IN_PORT可以解决问题0。&lt;/a>。&lt;/p>
&lt;p>虽然匹配了转发规则但是数据包却被丢弃的原因是OpenFlow 要求交换机不要把数据包从 ingress端口发出。貌似是为了避免转发环路出现。但是有时候确实有这个需求，而且不会造成问题。具体可以从链接中进去openflow论坛了解。&lt;/p>
&lt;h2 id="mininet配置网关">mininet配置网关&lt;/h2>
&lt;p>&lt;strong>ping不在一个网段的主机，包会转到子网的网关。而目前没有针对两个网段设置网关。&lt;/strong>&lt;/p>
&lt;pre>&lt;code class="language-bash">#修改host ip
py h1.setIP('10.0.0.11/24')
py h2.setIP('10.0.1.22/24')
#查看路由表
h1 netstat –rn或route(慢)
#给h1配置默认网关（ip）
h1 route add default gw 10.0.0.1 (h1-eth0)
#给h1配置静态arp表(默认网关的mac填host2)
h1 arp -s 10.0.0.1 00:00:00:00:00:02
h2 route add default gw 10.0.1.1
h2 arp -s 10.0.1.1 00:00:00:00:00:01
#查看arp表
h1 arp -a或-n
&lt;/code>&lt;/pre>
&lt;p>sudo tcpdump -e -vv -i s1-eth1（-vv详细报文信息，-e显示mac，–i指定监听的端口）&lt;/p>
&lt;p>h1 ping –c 1 h2
由于不是一个网段的，icmp报文的mac填的是网关的mac。&lt;/p>
&lt;pre>&lt;code class="language-bash">清空arp
h1 arp -n|awk '/^[1-9]/{print &amp;quot;arp -d &amp;quot; $1}'|sh -x
h2 arp -n|awk '/^[1-9]/{print &amp;quot;arp -d &amp;quot; $1}'|sh -x
&lt;/code>&lt;/pre>
&lt;p>ping不通是因为
&lt;strong>h1-eth0会发送ARP包获取网关的mac，而当前网关地址不存在对应设备，因此ICMP封包前获取不到目标地址信息，而导致网络不可达。&lt;/strong>&lt;/p>
&lt;p>（如果实验连接了控制器，可以通过控制器获得目标地址的信息，如OpenDaylight利用它预设的ARP Proxy。）&lt;/p>
&lt;p>让ovs的两个端口成为网关&lt;/p>
&lt;pre>&lt;code class="language-bash">mininet&amp;gt; s1 ifconfig s1-eth1 10.0.0.1/24
mininet&amp;gt; s1 ifconfig s1-eth2 10.0.1.1/24
&lt;/code>&lt;/pre>
&lt;p>配置流表让arp和icmp消息得到转发&lt;/p>
&lt;pre>&lt;code class="language-bash">处理ARP请求
当网管的ARP流到来后，将其交给本地的OVS处理。
mininet&amp;gt; sh ovs-ofctl add-flow s1 &amp;quot;table=0,priority=65535,arp,arp_tpa=10.0.0.1 actions=LOCAL&amp;quot;
mininet&amp;gt; sh ovs-ofctl add-flow s1 &amp;quot;table=0,priority=65535,arp,arp_tpa=10.0.1.1 actions=LOCAL&amp;quot;
处理ARP应答
该应答回复目标地址的出口，比如将目标网络10.0.0.1的包通过出口端口1抓发出去。端口信息可以查询如下：
mininet&amp;gt; sh ovs-vsctl -- --columns=name,ofport list Interface
name : &amp;quot;s1-eth2&amp;quot;
ofport : 2
name : &amp;quot;s1-eth1&amp;quot;
ofport : 1
name : &amp;quot;s1&amp;quot;
ofport : 65534
处理应答的流表如下：
mininet&amp;gt; sh ovs-ofctl add-flow s1 &amp;quot;table=0,priority=1,arp,nw_dst=10.0.0.1,actions=output:1&amp;quot;
mininet&amp;gt; sh ovs-ofctl add-flow s1 &amp;quot;table=0,priority=1,arp,nw_dst=20.0.0.1,actions=output:2&amp;quot;
对ICMP的请求进行处理。
为了使得流表表达更清晰，我们将ICMP路由的处理放在另外一个table处理。 也就是在table0中设置一个最低优先级的流，将非ARP的包丢给下一个流表处理。
mininet&amp;gt; sh ovs-ofctl add-flow s1 &amp;quot;table=0,priority=0,actions=resubmit(,1)&amp;quot;
在table(1)中，OVS的角色有点像router，我们需要修改ICMP封包的目标MAC地址。
mininet&amp;gt; sh ovs-ofctl add-flow s1 &amp;quot;table=1,icmp,nw_dst=10.0.0.1,actions=mod_dl_dst=00:00:00:00:00:01,output:1&amp;quot;
mininet&amp;gt; sh ovs-ofctl add-flow s1 &amp;quot;table=1,icmp,nw_dst=20.0.0.1,actions=mod_dl_dst=00:00:00:00:00:02,output:2&amp;quot;
&lt;/code>&lt;/pre>
&lt;p>##用Linux命令搭建sdn网络&lt;/p>
&lt;pre>&lt;code class="language-bash"># Create host namespaces
ip netns add h1
ip netns add h2
# Create switch
ovs-vsctl add-br s1
# Create links
ip link add h1-eth0 type veth peer name s1-eth1
ip link add h2-eth0 type veth peer name s1-eth2
ip link show
# Move host ports into namespaces
ip link set h1-eth0 netns h1
ip link set h2-eth0 netns h2
ip netns exec h1 ip link show
ip netns exec h2 ip link show
# Connect switch ports to OVS
ovs-vsctl add-port s1 s1-eth1
ovs-vsctl add-port s1 s1-eth2
ovs-vsctl show
# Set up OpenFlow controller
ovs-vsctl set-controller s1 tcp:127.0.0.1
ovs-controller ptcp: &amp;amp;
ovs-vsctl show
# Configure network
ip netns exec h1 ifconfig h1-eth0 10.1
ip netns exec h1 ifconfig lo up
ip netns exec h2 ifconfig h2-eth0 10.2
ip netns exec h2 ifconfig lo up
ifconfig s1-eth1 up
ifconfig s1-eth2 up
# Test network
ip netns exec h1 ping -c1 10.2
&lt;/code>&lt;/pre>
&lt;h2 id="mininet主机互相发包">mininet主机互相发包&lt;/h2>
&lt;p>在mininet/mininet/中的net.py中添加函数：&lt;/p>
&lt;pre>&lt;code class="language-python"> def sendMulti(self, number, period, times):
times=int(times)
#os.system(&amp;quot;ls&amp;quot;)
while (times):
base_port = 5001
host_list = [h for h in self.hosts]
_len = len(host_list)
for i in range(0, _len):
client = host_list[i]
for j in range(0, _len):
server = host_list[j]
if (i != j):
client.cmd(&amp;quot;/home/cjx/ubuntu/mininet/sends&amp;quot; + ' ' + server.IP() + ' ' + number + ' ' + period)
base_port += 1
times = times - 1
&lt;/code>&lt;/pre>
&lt;p>在mininet/mininet/中的cli.py中添加函数：&lt;/p>
&lt;pre>&lt;code class="language-python">def do_sendmulti(self, line):
&amp;quot;&amp;quot;&amp;quot;Multi iperf UDP test between nodes&amp;quot;&amp;quot;&amp;quot;
args = line.split()
if len(args) == 3:
udpBw = args[0]
period = args[1]
times = args[2]
err = False
self.mn.sendMulti(udpBw, period,times)
else:
error('invalid number of args: sendmulti udpBw \n' +
'udpBw examples: 1M\n')
&lt;/code>&lt;/pre>
&lt;p>在mininet/bin/mn文件中两处修改：&lt;/p>
&lt;pre>&lt;code class="language-python">TESTS = { name: True
for name in ( 'pingall', 'pingpair', 'iperf', 'iperfudp', 'sendmulti' ) }
ALTSPELLING = { 'pingall': 'pingAll', 'pingpair': 'pingPair',
'iperfudp': 'iperfUdp',
'sendmulti': 'sendMulti'}
&lt;/code>&lt;/pre>
&lt;p>send.c发包源文件编译成执行文件&lt;/p>
&lt;pre>&lt;code class="language-c">int main(int argc, char** argv)
{
///必须有目的ip的参数
if(argc != 4)
{
printf(&amp;quot;usage: send [ip] [number] [time interval(ms)]\n&amp;quot;);
return -1;
}
///根据参数组合出目的主机的ip地址
char dst_ip[20];
sprintf(dst_ip,&amp;quot;%s&amp;quot;,argv[1]);
int times = atoi(argv[2]);
int time_interval = atoi(argv[3]);
time_interval*=1000;
/// 初始化本机地址
struct sockaddr_in my_addr;
memset(&amp;amp;my_addr,0,sizeof(my_addr));
my_addr.sin_family = AF_INET; //Address family 指定为ipv4
my_addr.sin_addr.s_addr = INADDR_ANY; ////INADDR_ANY表示自动获取本机地址
my_addr.sin_port = htons(8889); ///端口号（本机的端口好表示用于发送的端口号）
///初始化目的主机地址
struct sockaddr_in dst_addr;
memset(&amp;amp;dst_addr,0,sizeof(dst_addr));
dst_addr.sin_family = AF_INET;
dst_addr.sin_addr.s_addr = inet_addr(dst_ip);
dst_addr.sin_port = htons(8888);
///创建套接字
int sockfd = socket(AF_INET,SOCK_DGRAM,0);
if(sockfd == -1)
{
perror(&amp;quot;create socket failed\n&amp;quot;);
return -1;
}
///绑定端口
if(bind(sockfd,(struct sockaddr*)&amp;amp;my_addr,sizeof(my_addr)) == -1)
{
perror(&amp;quot;bind failed\n&amp;quot;);
return -1;
}
unsigned char buf[16];
int i = 0;
for(i=0; i&amp;lt; 16; i++)
{
buf[i] = 0xff;
}
///发送数据包
for(i=0; i&amp;lt;times; i++)
{
usleep(time_interval);
// sleep(1);
printf(&amp;quot;sending packet %d to %s\n&amp;quot;,i,dst_ip);
if(sendto(sockfd,buf,sizeof(buf),0,(struct sockaddr*)&amp;amp;dst_addr,sizeof(dst_addr)) == -1)
{
perror(&amp;quot;send failed\n&amp;quot;);
return -1;
}
}
close(sockfd);
}
&lt;/code>&lt;/pre>
&lt;p>Did you find this page helpful? Consider sharing it 🙌&lt;/p></description></item><item><title>RouteFlow</title><link>/post/routeflow/</link><pubDate>Wed, 07 Oct 2020 00:00:00 +0000</pubDate><guid>/post/routeflow/</guid><description>&lt;h1 id="routeflow">Routeflow&lt;/h1>
&lt;p>在openflow网络之上提供虚拟IP路由服务的平台。它依赖于POX,OpenFlow,MongoDB,OVS,Quagga&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/routeflow/RouteFlow">https://github.com/routeflow/RouteFlow&lt;/a> 官方代码&lt;/li>
&lt;/ul>
&lt;h2 id="第三方代码">第三方代码&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="https://github.com/chesteve/RouteFlowVM">https://github.com/chesteve/RouteFlowVM&lt;/a> 虚拟机&lt;/li>
&lt;li>&lt;a href="https://github.com/call518/SDN-TEST">https://github.com/call518/SDN-TEST&lt;/a> ONOS OpenDaylight Routeflowd等&lt;/li>
&lt;li>&lt;a href="https://github.com/hungys/RoutingFlow(">https://github.com/hungys/RoutingFlow(&lt;/a>在Ryu上实现RIP,OSPF路由协议)&lt;/li>
&lt;/ul>
&lt;h2 id="三大核心组件">三大核心组件&lt;/h2>
&lt;ul>
&lt;li>&lt;strong>RFClient&lt;/strong> : 运行在VM（准确说是lxc容器）中的守护程序，监测Linux ARP ,路由表的改变。如果有更新，路由信息会被发送给RFserver。&lt;/li>
&lt;li>&lt;strong>RFServer&lt;/strong> : 一个独立的app，用来连接并管理运行在VM中的RFClient实例。RFserver负责维护RFclient实例、接口和对应交换机、端口的映射关系。它也和与RFproxy连接，来配置流表&lt;/li>
&lt;li>&lt;strong>RFProxy&lt;/strong> ：POX的一个app,通过openflow协议与ovs交互，监听RFserver的指令，&lt;/li>
&lt;/ul>
&lt;h2 id="vmrfclient和ovs的映射关系">VM（RFclient）和OVS的映射关系&lt;/h2>
&lt;p>配置被存储在MongoDB一个集合中&lt;/p>
&lt;ul>
&lt;li>VM 和OVS的映射关系&lt;/li>
&lt;li>virtual interface和交换机端口的映射关系beyond 1:1&lt;/li>
&lt;li>多个控制器管理网络&lt;/li>
&lt;/ul>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>格式&lt;/th>
&lt;th>类型&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>vm id, vm port, -, -, -, -, -&lt;/td>
&lt;td>闲置的client port&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>-, -, -, -, dp id, dp port, ct id&lt;/td>
&lt;td>闲置的datapath port&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>vm id, vm port, dp id, dp port, -, -, ct id&lt;/td>
&lt;td>client-datapath映射关系&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>vm id, vm port, dp id, dp port, vs id, vs port, ct id&lt;/td>
&lt;td>激活的 client-datapath&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>其他与虚拟交换机相关的域 (vs_*)在运行时定义. The ct_id field 标识交换机连接的控制器.(此机制允许多控制器）&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>状态图：
&lt;img src="https://raw.githubusercontent.com/wiki/routeflow/RouteFlow/images/rfstates.png" align="center" >&lt;/p>
&lt;p>当交换机连入网络，RFproxy会告知RFserver交换机的每个物理端口。这些datapath端口被server注册作为空闲的datapath端口或作为client-datapath关联；前者发生的条件是要么&lt;strong>正在被注册的datapath端口没有configuration&lt;/strong>，要么&lt;strong>已配置的client端口（与此datapath端口关联）尚未注册&lt;/strong>。 后者发生的条件是在&lt;strong>client端口（基于the configuration与此datapath关联）被注册为空闲&lt;/strong>。&lt;/p>
&lt;p>当一个RFclient启动，他会告诉server它的端口。这些client端口被server注册作为空闲的client端口或client-datapath关联。和上文的datapath端口行为描述类似。&lt;/p>
&lt;p>关联之后，RFserver要求client触发一个消息，这个消息将通过client连接的虚拟交换机到达proxy。这样proxy能感知client和虚拟交换机之间的连接，通知server。
然后server决定怎么处理这个信息。一般地，proxy会被命令重定向所有从虚拟交换机到与它关联的物理交换机的流，and vice-versa（反之亦然）.&lt;/p>
&lt;p>当交换机断开网络，它的端口从所有涉及到的关联中移除，留下空闲的client端口。如果datapath回来了，系统将把它当作新的datapath，如上文描述.&lt;/p>
&lt;h3 id="lxc简介">LXC简介&lt;/h3>
&lt;p>Linux Container容器是一种内核虚拟化技术，简称LXC。提供轻量级的虚拟化，以便隔离进程和资源，使用 LXC 的优点就是不需要安装太多的软件包，使用过程也不会占用太多的资源。LXC 在资源管理方面依赖 Linux 内核的 cgroups （Control Groups） 系统，cgroups 系统是 Linux 内核提供的一个基于进程组的资源管理的框架，可以为特定的进程组限定可以使用的资源。&lt;/p>
&lt;h3 id="lxc常用命令">lxc常用命令&lt;/h3>
&lt;pre>&lt;code class="language-sh">1. 安装lxc
apt-get -y --force-yes install lxc
2. 检查
lxc-checkconfig 如果所有项目显示“enabled”则OK
3. 根据模板创建容器
lxc-create -t ubuntu -n base
#-n是容器的名字;-t 是容器的模板（模板的保存路径是：/usr/lib/lxc/templates/==模板都是一个脚本文件，执行了一系列操作，包括创建容器的时候挂载文件系统，配置网络，安装必要软件，创建用户/属组，设置密码等=
4. 启动 LXC 虚拟计算机
lxc-start -n testA 确认账号和密码后登录虚拟计算机
6、列出当前所有的容器
　　lxc-ls
7、使用 console 登入容器
lxc-console -n testA –t 3
8、停止运行一个容器
lxc-stop -n testA
9、获取一个容器的状态
lxc-info -n ol6ctr1
10、把一个容器销毁
lxc-destroy -n testA1
11、复制一个容器
lxc-clone -o testA -n ol6ctr2
12、暂停或恢复一个容器
lxc-freeze -n testA
lxc-unfreeze -n testA
13、修改 LXC 网络接口
vi /etc/default/lxc
14. 用户修改后要重新启动网络服务
service lxc-net restart
&lt;/code>&lt;/pre>
&lt;h2 id="实验">实验&lt;/h2>
&lt;h3 id="编译rfclient">编译rfclient&lt;/h3>
&lt;pre>&lt;code class="language-sh">make rfclient
&lt;/code>&lt;/pre>
&lt;p>#make build -&amp;gt; lib -&amp;gt; rfclient:&lt;br> 编译rflib/ipc, rflib/types, rfclient/. 目录下的文件到 build/&lt;/p>
&lt;pre>&lt;code class="language-sh">./Makefile
export ROOT_DIR=.
export BUILD_DIR=./build
export LIB_DIR=./rflib
export MONGO_DIR=/usr/local/include/mongo
# 编译产物存放目录
export BUILD_LIB_DIR=./build/lib
export BUILD_OBJ_DIR=./build/obj
export libdirs := ipc types （rflib下的两个目录）
&lt;/code>&lt;/pre>
&lt;h3 id="创建lxc容器切到rftest目录执行create">创建LXC容器：切到rftest目录,执行./create&lt;/h3>
&lt;p>create脚本内容：&lt;/p>
&lt;ol>
&lt;li>容器创建路径/var/lib/lxc&lt;/li>
&lt;li>apt安装lxc&lt;/li>
&lt;li>创建默认LXC容器base&lt;/li>
&lt;/ol>
&lt;pre>&lt;code>lxc-create -t ubuntu -n base
-n是容器的名字;-t 是容器的模板（在/usr/lib/lxc/templates/里，模板就是一个脚本文件，内容包括创建容器的时候挂载文件系统，配置网络，安装必要软件，创建用户/属组，设置密码等==）
&lt;/code>&lt;/pre>
&lt;ol start="4">
&lt;li>安装quagga、tcpdump到容器&lt;/li>
&lt;/ol>
&lt;pre>&lt;code class="language-sh">chroot $LXCDIR/base/rootfs apt-get install
#chroot命令用来在指定的根目录下运行指令
&lt;/code>&lt;/pre>
&lt;ol start="5">
&lt;li>基于刚刚创建的base容器和rftest/conifg下的容器配置文件，克隆其他LXC容器&lt;/li>
&lt;/ol>
&lt;h3 id="1rftest1">1)rftest1&lt;/h3>
&lt;p>创建3个LXC容器b1(172.31.1.2)、b2(172.31.2.2)、rfvm1（192.169.1.100）其中rfvm1分别配两个interface与b1,b2连接。设法使rfvm1充当网关（路由器），使得b1 能ping 通 b2。
&lt;img src="https://github.com/routeflow/RouteFlow/wiki/images/rftest1_scenario.png" alt="image">&lt;/p>
&lt;p>RF中的一台虚拟机充当openflow交换机，这台虚拟机中路由表和arp表发生任何事，都被复制到交换机。 RFClient负责监听这些表的事件并且通知rfserver。
&lt;br>因此要部署一台LXC容器,另配一个iface作为管理接口连接这个RFclient和其他&lt;br>
&lt;strong>config文件:&lt;/strong>
此文件是所有setup的基础，根据情况修改名字和hwaddr&lt;br>
第一个iface(rfvm1.0)的hwaddr将被用为RFclient的ID.&lt;/p>
&lt;pre>&lt;code class="language-sh">lxc.utsname = rfvm1
lxc.network.type = veth
lxc.network.flags = up
lxc.network.hwaddr = 12:a0:a0:a0:a0:a0
lxc.network.link=lxcbr0
lxc.network.type = veth
lxc.network.flags = up
lxc.network.veth.pair= rfvm1.1
lxc.network.hwaddr = 12:a1:a1:a1:a1:a1
lxc.network.type = veth
lxc.network.flags = up
lxc.network.veth.pair = rfvm1.2
lxc.network.hwaddr = 12:a2:a2:a2:a2:a2
...
lxc.rootfs = /var/lib/lxc/rfvm1/rootfs
lxc.mount = /var/lib/lxc/rfvm1/fstab
&lt;/code>&lt;/pre>
&lt;p>rootfs和mount不用修改。容器的位置一般在 &lt;strong>/var/lib/lxc&lt;/strong>
&lt;strong>/etc/sysctl.conf&lt;/strong> 中开启IPv4转发，让rfvm1作为网关，关闭quagga
&lt;strong>/etc/rc.local&lt;/strong>调用&lt;strong>root/run_rfclient脚本&lt;/strong>开启RFclient实例。&lt;/p>
&lt;p>==ReadMe==：All these customizations, when structured like in the rftest/config/rfvm1 folder will be read the the rftest/create script, that will create any number of LXC containers based on a basic template.&lt;/p>
&lt;h4 id="执行rftest1">执行./rftest1&lt;/h4>
&lt;p>此脚本必须被root用户执行。&lt;br>
一、重置环境:&lt;/p>
&lt;pre>&lt;code class="language-sh">reset 1：
初始化ovs:删除dp0,switch1网桥
初始化VMs：lxc关机，删除vm路径下的rootfs/var/run/network/ifstate
断开mongodb，删除数据rfvm1路径下的rootfs/opt/rfclient
trap &amp;quot;reset 0; exit 0&amp;quot; INT
#当脚本程序收到INT命令（被中断时），执行reset 0; exit 0完成清理工作
配置容器的管理接口：ifconfig lxcbr0 $MONGODB_ADDR(192.168.10.1) up
sed -i &amp;quot;/bind_ip/c\bind_ip = 127.0.0.1,$MONGODB_ADDR&amp;quot; $MONGODB_CON
#用bind_ip=127.0.0.1,192.168.10.1取代bind_ip行
重启mongodb
wait_port_listen $MONGODB_PORT(27017)
# 阻塞执行直到在指定端口监听到一个socket开启。
# MongoDB服务器：监听此网桥，确保容器能连接到 Routeflow架构。
&lt;/code>&lt;/pre>
&lt;p>二、配置LXC 容器&lt;strong>rfvm1&lt;/strong>作为RFclient运行&lt;/p>
&lt;pre>&lt;code>新建rfclient目录
mkdir /var/lib/lxc/rfvm1/rootfs/opt/rfclient
复制rfclient可执行程序
cp build/rfclient /var/lib/lxc/rfvm1/rootfs/opt/rfclient/rfclient
创建脚本run_rfclient.sh在/var/lib/lxc/rfvm1/rootfs/root下用于启动RFclient （脚本内容：sleep 5
/opt/rfclient/rfclient&amp;gt; /var/log/rfclient.log）
启动pox
#作为控制器app的RFproxy也随之启动,控制器日志等级设为INFO
wait_port_listen $Contorller_PORT(6633)
启动rfserver
./rfserver/rfserver.py rftest/rftest1config.csv &amp;amp;
# rftest1config.csv是vm_id,vm_port,ct_id,dp_id,dp_port映射表
启动容器并登录：lxc-start -n rfvm1 -d
&lt;/code>&lt;/pre>
&lt;p>三、连接容器到正在运行数据库服务的hosts&lt;/p>
&lt;pre>&lt;code class="language-sh">ovs添加网桥dp0，端口rfvm1.1，rfvm1.2,dpid=7266767372667673
dp0连接控制器：ovs-vsctl set-controller dp0 tcp:127.0.0.1:$CONTROLLER_PORT（6633）
启动hosts（容器b1、b2)
ovs添加网桥switch1,端口b1.0、b2.0，dpid=99
switch1连接到控制器
&lt;/code>&lt;/pre>
&lt;p>在host创建ovs网桥，连接&lt;em>容器的管理接口&lt;/em>（rfvm1的eth0）
&lt;strong>br0&lt;/strong>，192.169.1.1，这个地址被写死在rflib/defs.h（给RFclient和NOX-RFproxy以及RFserver和POX-RFproxy）
四、登录到容器host1
lxc-console -n b1（用户名密码都是ubuntu）
在b1（172.31.1.2）里ping -c 3 172.31.2.2（b2）&lt;/p>
&lt;p>ping通的原因：
rfserver维护映射表&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>vm_id&lt;/th>
&lt;th>vm_port&lt;/th>
&lt;th>ct_id&lt;/th>
&lt;th>dp_id,dp_port&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>12A0A0A0A0A0&lt;/td>
&lt;td>1&lt;/td>
&lt;td>0&lt;/td>
&lt;td>99&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>12A0A0A0A0A0&lt;/td>
&lt;td>2&lt;/td>
&lt;td>0&lt;/td>
&lt;td>99&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>容器id是12A0A0A0A0A0，它的接口eth1对应dpid（switch）的接口1,&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>接口eth2对应dpid（switch）的接口2&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;td>&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>Did you find this page helpful? Consider sharing it 🙌&lt;/p></description></item></channel></rss>